{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rotation_Equivariant_CNN_Ensembles.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wa_knoiCf_g",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Import useful packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQq2SdEqCWJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models.resnet import ResNet, BasicBlock\n",
        "from torchvision.datasets import MNIST\n",
        "from tqdm.autonotebook import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import inspect\n",
        "import time\n",
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-BsCZFFHAKE",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Load and normalizing the MNIST training datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzCJEVmsHAfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "means = deviations = [0.5]\n",
        "train_transform=[]\n",
        "start_angle=-270\n",
        "end_angle=0\n",
        "ensemble_num=7\n",
        "for i in range(ensemble_num):\n",
        "  train_transform.append(transforms.Compose([transforms.RandomRotation([start_angle,end_angle]),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize(means, deviations)]))\n",
        "  start_angle=-210+60*i\n",
        "  end_angle=60+60*i\n",
        "\n",
        "# add trainset \n",
        "trainset=[]\n",
        "for i in range(ensemble_num):\n",
        "  trainset.append(torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform[i]))\n",
        "# add trainloader\n",
        "trainloader=[]\n",
        "for i in range(ensemble_num):\n",
        "    trainloader.append(torch.utils.data.DataLoader(trainset[i], batch_size=128,\n",
        "                                          shuffle=True, num_workers=2))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A1_RXaV3vLc",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Define a Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4in2zxJ23vrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MnistResNet(nn.Module):  \n",
        "    def __init__(self):\n",
        "        super(MnistResNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.drop_out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU-HThkx4ubE",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Define Loss functions and optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-JJT8oj4utW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# use GPU\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# define models\n",
        "model=[]\n",
        "for i in range(ensemble_num):\n",
        "  model.append(MnistResNet().to(device))\n",
        "  \n",
        "# define optimizers\n",
        "optimizer=[]\n",
        "for i in range(ensemble_num):\n",
        "  optimizer.append(optim.SGD(model[i].parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq7IC55F8c91",
        "colab_type": "text"
      },
      "source": [
        "## Step 5ï¼šTrain each ensemble network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zymEtbj78dUx",
        "colab_type": "code",
        "outputId": "0c366168-1b9b-451b-df08-052c7a234b33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epoch_range = 5\n",
        "# training loop + eval loop\n",
        "for ensemble_id in range(ensemble_num):\n",
        "    running_loss = 0.0\n",
        "    print(\"Loss of ensemble model\",ensemble_id)\n",
        "    for epoch in range(epoch_range):\n",
        "      for i, data in enumerate(trainloader[ensemble_id], 0):\n",
        "          # get the inputs\n",
        "          inputs, labels = data\n",
        "  #         print(labels.numpy().shape)\n",
        "\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer[ensemble_id].zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = model[ensemble_id](inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer[ensemble_id].step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 20 == 19:    # print every 2000 mini-batches\n",
        "              print('[%d, %5d] loss: %.6f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 2000))\n",
        "              running_loss = 0.0"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss of ensemble model 0\n",
            "[1,    20] loss: 0.022667\n",
            "[1,    40] loss: 0.020211\n",
            "[1,    60] loss: 0.016110\n",
            "[1,    80] loss: 0.013261\n",
            "[1,   100] loss: 0.011610\n",
            "[1,   120] loss: 0.010357\n",
            "[1,   140] loss: 0.009037\n",
            "[1,   160] loss: 0.008453\n",
            "[1,   180] loss: 0.007826\n",
            "[1,   200] loss: 0.007736\n",
            "[1,   220] loss: 0.007244\n",
            "[1,   240] loss: 0.006743\n",
            "[1,   260] loss: 0.006469\n",
            "[1,   280] loss: 0.006212\n",
            "[1,   300] loss: 0.006362\n",
            "[1,   320] loss: 0.006155\n",
            "[1,   340] loss: 0.005942\n",
            "[1,   360] loss: 0.005928\n",
            "[1,   380] loss: 0.005273\n",
            "[1,   400] loss: 0.005191\n",
            "[1,   420] loss: 0.005156\n",
            "[1,   440] loss: 0.005242\n",
            "[1,   460] loss: 0.004593\n",
            "[2,    20] loss: 0.007382\n",
            "[2,    40] loss: 0.004687\n",
            "[2,    60] loss: 0.004983\n",
            "[2,    80] loss: 0.004614\n",
            "[2,   100] loss: 0.004995\n",
            "[2,   120] loss: 0.004498\n",
            "[2,   140] loss: 0.004322\n",
            "[2,   160] loss: 0.004120\n",
            "[2,   180] loss: 0.004342\n",
            "[2,   200] loss: 0.003989\n",
            "[2,   220] loss: 0.003778\n",
            "[2,   240] loss: 0.004091\n",
            "[2,   260] loss: 0.004126\n",
            "[2,   280] loss: 0.004072\n",
            "[2,   300] loss: 0.003855\n",
            "[2,   320] loss: 0.003796\n",
            "[2,   340] loss: 0.004085\n",
            "[2,   360] loss: 0.003488\n",
            "[2,   380] loss: 0.003730\n",
            "[2,   400] loss: 0.003697\n",
            "[2,   420] loss: 0.003317\n",
            "[2,   440] loss: 0.003470\n",
            "[2,   460] loss: 0.003681\n",
            "[3,    20] loss: 0.005256\n",
            "[3,    40] loss: 0.003549\n",
            "[3,    60] loss: 0.003369\n",
            "[3,    80] loss: 0.003461\n",
            "[3,   100] loss: 0.003700\n",
            "[3,   120] loss: 0.003681\n",
            "[3,   140] loss: 0.003544\n",
            "[3,   160] loss: 0.003464\n",
            "[3,   180] loss: 0.003233\n",
            "[3,   200] loss: 0.003522\n",
            "[3,   220] loss: 0.003186\n",
            "[3,   240] loss: 0.003256\n",
            "[3,   260] loss: 0.003499\n",
            "[3,   280] loss: 0.003167\n",
            "[3,   300] loss: 0.003444\n",
            "[3,   320] loss: 0.003379\n",
            "[3,   340] loss: 0.003473\n",
            "[3,   360] loss: 0.003592\n",
            "[3,   380] loss: 0.003283\n",
            "[3,   400] loss: 0.003125\n",
            "[3,   420] loss: 0.003622\n",
            "[3,   440] loss: 0.002821\n",
            "[3,   460] loss: 0.003314\n",
            "Loss of ensemble model 1\n",
            "[1,    20] loss: 0.022683\n",
            "[1,    40] loss: 0.020567\n",
            "[1,    60] loss: 0.015977\n",
            "[1,    80] loss: 0.013481\n",
            "[1,   100] loss: 0.011361\n",
            "[1,   120] loss: 0.009699\n",
            "[1,   140] loss: 0.008954\n",
            "[1,   160] loss: 0.007738\n",
            "[1,   180] loss: 0.007421\n",
            "[1,   200] loss: 0.007580\n",
            "[1,   220] loss: 0.006928\n",
            "[1,   240] loss: 0.006832\n",
            "[1,   260] loss: 0.006552\n",
            "[1,   280] loss: 0.006036\n",
            "[1,   300] loss: 0.006113\n",
            "[1,   320] loss: 0.005742\n",
            "[1,   340] loss: 0.005688\n",
            "[1,   360] loss: 0.005397\n",
            "[1,   380] loss: 0.005397\n",
            "[1,   400] loss: 0.005007\n",
            "[1,   420] loss: 0.005025\n",
            "[1,   440] loss: 0.004936\n",
            "[1,   460] loss: 0.004566\n",
            "[2,    20] loss: 0.006578\n",
            "[2,    40] loss: 0.004846\n",
            "[2,    60] loss: 0.004549\n",
            "[2,    80] loss: 0.004873\n",
            "[2,   100] loss: 0.004638\n",
            "[2,   120] loss: 0.004209\n",
            "[2,   140] loss: 0.004486\n",
            "[2,   160] loss: 0.004008\n",
            "[2,   180] loss: 0.004005\n",
            "[2,   200] loss: 0.004002\n",
            "[2,   220] loss: 0.004245\n",
            "[2,   240] loss: 0.004030\n",
            "[2,   260] loss: 0.003816\n",
            "[2,   280] loss: 0.003858\n",
            "[2,   300] loss: 0.003806\n",
            "[2,   320] loss: 0.003745\n",
            "[2,   340] loss: 0.003770\n",
            "[2,   360] loss: 0.003759\n",
            "[2,   380] loss: 0.003654\n",
            "[2,   400] loss: 0.003628\n",
            "[2,   420] loss: 0.003574\n",
            "[2,   440] loss: 0.003290\n",
            "[2,   460] loss: 0.003402\n",
            "[3,    20] loss: 0.005188\n",
            "[3,    40] loss: 0.003633\n",
            "[3,    60] loss: 0.003534\n",
            "[3,    80] loss: 0.003420\n",
            "[3,   100] loss: 0.003299\n",
            "[3,   120] loss: 0.003077\n",
            "[3,   140] loss: 0.003368\n",
            "[3,   160] loss: 0.003071\n",
            "[3,   180] loss: 0.003384\n",
            "[3,   200] loss: 0.003377\n",
            "[3,   220] loss: 0.003395\n",
            "[3,   240] loss: 0.003443\n",
            "[3,   260] loss: 0.003299\n",
            "[3,   280] loss: 0.003196\n",
            "[3,   300] loss: 0.003005\n",
            "[3,   320] loss: 0.003324\n",
            "[3,   340] loss: 0.003317\n",
            "[3,   360] loss: 0.002972\n",
            "[3,   380] loss: 0.003308\n",
            "[3,   400] loss: 0.003378\n",
            "[3,   420] loss: 0.002506\n",
            "[3,   440] loss: 0.003336\n",
            "[3,   460] loss: 0.003312\n",
            "Loss of ensemble model 2\n",
            "[1,    20] loss: 0.022616\n",
            "[1,    40] loss: 0.019418\n",
            "[1,    60] loss: 0.015475\n",
            "[1,    80] loss: 0.013395\n",
            "[1,   100] loss: 0.011713\n",
            "[1,   120] loss: 0.009740\n",
            "[1,   140] loss: 0.008915\n",
            "[1,   160] loss: 0.008700\n",
            "[1,   180] loss: 0.007782\n",
            "[1,   200] loss: 0.007657\n",
            "[1,   220] loss: 0.007083\n",
            "[1,   240] loss: 0.006724\n",
            "[1,   260] loss: 0.006732\n",
            "[1,   280] loss: 0.006691\n",
            "[1,   300] loss: 0.005995\n",
            "[1,   320] loss: 0.006116\n",
            "[1,   340] loss: 0.006002\n",
            "[1,   360] loss: 0.005600\n",
            "[1,   380] loss: 0.005197\n",
            "[1,   400] loss: 0.005255\n",
            "[1,   420] loss: 0.005127\n",
            "[1,   440] loss: 0.005248\n",
            "[1,   460] loss: 0.004910\n",
            "[2,    20] loss: 0.006958\n",
            "[2,    40] loss: 0.004726\n",
            "[2,    60] loss: 0.004435\n",
            "[2,    80] loss: 0.004762\n",
            "[2,   100] loss: 0.004375\n",
            "[2,   120] loss: 0.004123\n",
            "[2,   140] loss: 0.004368\n",
            "[2,   160] loss: 0.004193\n",
            "[2,   180] loss: 0.004339\n",
            "[2,   200] loss: 0.004411\n",
            "[2,   220] loss: 0.004593\n",
            "[2,   240] loss: 0.004108\n",
            "[2,   260] loss: 0.004022\n",
            "[2,   280] loss: 0.003898\n",
            "[2,   300] loss: 0.004010\n",
            "[2,   320] loss: 0.003785\n",
            "[2,   340] loss: 0.003667\n",
            "[2,   360] loss: 0.003744\n",
            "[2,   380] loss: 0.003666\n",
            "[2,   400] loss: 0.003657\n",
            "[2,   420] loss: 0.003904\n",
            "[2,   440] loss: 0.003771\n",
            "[2,   460] loss: 0.003854\n",
            "[3,    20] loss: 0.005560\n",
            "[3,    40] loss: 0.003723\n",
            "[3,    60] loss: 0.003297\n",
            "[3,    80] loss: 0.003493\n",
            "[3,   100] loss: 0.003481\n",
            "[3,   120] loss: 0.003448\n",
            "[3,   140] loss: 0.003113\n",
            "[3,   160] loss: 0.003281\n",
            "[3,   180] loss: 0.003585\n",
            "[3,   200] loss: 0.003202\n",
            "[3,   220] loss: 0.003211\n",
            "[3,   240] loss: 0.003259\n",
            "[3,   260] loss: 0.003363\n",
            "[3,   280] loss: 0.003212\n",
            "[3,   300] loss: 0.003375\n",
            "[3,   320] loss: 0.003413\n",
            "[3,   340] loss: 0.003526\n",
            "[3,   360] loss: 0.003539\n",
            "[3,   380] loss: 0.003329\n",
            "[3,   400] loss: 0.002963\n",
            "[3,   420] loss: 0.002947\n",
            "[3,   440] loss: 0.003033\n",
            "[3,   460] loss: 0.003069\n",
            "Loss of ensemble model 3\n",
            "[1,    20] loss: 0.022732\n",
            "[1,    40] loss: 0.020620\n",
            "[1,    60] loss: 0.016563\n",
            "[1,    80] loss: 0.013576\n",
            "[1,   100] loss: 0.012454\n",
            "[1,   120] loss: 0.010876\n",
            "[1,   140] loss: 0.009494\n",
            "[1,   160] loss: 0.008613\n",
            "[1,   180] loss: 0.008141\n",
            "[1,   200] loss: 0.007718\n",
            "[1,   220] loss: 0.007582\n",
            "[1,   240] loss: 0.007406\n",
            "[1,   260] loss: 0.007130\n",
            "[1,   280] loss: 0.006424\n",
            "[1,   300] loss: 0.006018\n",
            "[1,   320] loss: 0.006132\n",
            "[1,   340] loss: 0.005672\n",
            "[1,   360] loss: 0.005996\n",
            "[1,   380] loss: 0.005779\n",
            "[1,   400] loss: 0.005366\n",
            "[1,   420] loss: 0.005077\n",
            "[1,   440] loss: 0.005025\n",
            "[1,   460] loss: 0.004854\n",
            "[2,    20] loss: 0.006939\n",
            "[2,    40] loss: 0.004373\n",
            "[2,    60] loss: 0.004803\n",
            "[2,    80] loss: 0.004737\n",
            "[2,   100] loss: 0.004506\n",
            "[2,   120] loss: 0.004571\n",
            "[2,   140] loss: 0.004236\n",
            "[2,   160] loss: 0.004713\n",
            "[2,   180] loss: 0.004479\n",
            "[2,   200] loss: 0.004326\n",
            "[2,   220] loss: 0.004229\n",
            "[2,   240] loss: 0.004412\n",
            "[2,   260] loss: 0.004050\n",
            "[2,   280] loss: 0.004152\n",
            "[2,   300] loss: 0.004208\n",
            "[2,   320] loss: 0.003828\n",
            "[2,   340] loss: 0.003744\n",
            "[2,   360] loss: 0.003674\n",
            "[2,   380] loss: 0.003592\n",
            "[2,   400] loss: 0.003886\n",
            "[2,   420] loss: 0.003814\n",
            "[2,   440] loss: 0.003773\n",
            "[2,   460] loss: 0.003558\n",
            "[3,    20] loss: 0.005214\n",
            "[3,    40] loss: 0.003673\n",
            "[3,    60] loss: 0.003417\n",
            "[3,    80] loss: 0.003775\n",
            "[3,   100] loss: 0.003757\n",
            "[3,   120] loss: 0.003603\n",
            "[3,   140] loss: 0.003805\n",
            "[3,   160] loss: 0.003436\n",
            "[3,   180] loss: 0.003813\n",
            "[3,   200] loss: 0.003383\n",
            "[3,   220] loss: 0.003650\n",
            "[3,   240] loss: 0.003020\n",
            "[3,   260] loss: 0.003756\n",
            "[3,   280] loss: 0.003252\n",
            "[3,   300] loss: 0.003257\n",
            "[3,   320] loss: 0.003091\n",
            "[3,   340] loss: 0.003106\n",
            "[3,   360] loss: 0.003733\n",
            "[3,   380] loss: 0.003081\n",
            "[3,   400] loss: 0.003201\n",
            "[3,   420] loss: 0.003224\n",
            "[3,   440] loss: 0.002932\n",
            "[3,   460] loss: 0.003233\n",
            "Loss of ensemble model 4\n",
            "[1,    20] loss: 0.022825\n",
            "[1,    40] loss: 0.020486\n",
            "[1,    60] loss: 0.016519\n",
            "[1,    80] loss: 0.013740\n",
            "[1,   100] loss: 0.012045\n",
            "[1,   120] loss: 0.010630\n",
            "[1,   140] loss: 0.009185\n",
            "[1,   160] loss: 0.008804\n",
            "[1,   180] loss: 0.008240\n",
            "[1,   200] loss: 0.007982\n",
            "[1,   220] loss: 0.007413\n",
            "[1,   240] loss: 0.007093\n",
            "[1,   260] loss: 0.006776\n",
            "[1,   280] loss: 0.006712\n",
            "[1,   300] loss: 0.006319\n",
            "[1,   320] loss: 0.005738\n",
            "[1,   340] loss: 0.005878\n",
            "[1,   360] loss: 0.005880\n",
            "[1,   380] loss: 0.005747\n",
            "[1,   400] loss: 0.005313\n",
            "[1,   420] loss: 0.005134\n",
            "[1,   440] loss: 0.004498\n",
            "[1,   460] loss: 0.004700\n",
            "[2,    20] loss: 0.006817\n",
            "[2,    40] loss: 0.004758\n",
            "[2,    60] loss: 0.004749\n",
            "[2,    80] loss: 0.004627\n",
            "[2,   100] loss: 0.004530\n",
            "[2,   120] loss: 0.004354\n",
            "[2,   140] loss: 0.004329\n",
            "[2,   160] loss: 0.004308\n",
            "[2,   180] loss: 0.004175\n",
            "[2,   200] loss: 0.004316\n",
            "[2,   220] loss: 0.003995\n",
            "[2,   240] loss: 0.004180\n",
            "[2,   260] loss: 0.003915\n",
            "[2,   280] loss: 0.003971\n",
            "[2,   300] loss: 0.003927\n",
            "[2,   320] loss: 0.003914\n",
            "[2,   340] loss: 0.003813\n",
            "[2,   360] loss: 0.003992\n",
            "[2,   380] loss: 0.003733\n",
            "[2,   400] loss: 0.003709\n",
            "[2,   420] loss: 0.003623\n",
            "[2,   440] loss: 0.003642\n",
            "[2,   460] loss: 0.003631\n",
            "[3,    20] loss: 0.005233\n",
            "[3,    40] loss: 0.003533\n",
            "[3,    60] loss: 0.003427\n",
            "[3,    80] loss: 0.003305\n",
            "[3,   100] loss: 0.003595\n",
            "[3,   120] loss: 0.003250\n",
            "[3,   140] loss: 0.003342\n",
            "[3,   160] loss: 0.003656\n",
            "[3,   180] loss: 0.003410\n",
            "[3,   200] loss: 0.003621\n",
            "[3,   220] loss: 0.003182\n",
            "[3,   240] loss: 0.003528\n",
            "[3,   260] loss: 0.003627\n",
            "[3,   280] loss: 0.003598\n",
            "[3,   300] loss: 0.003102\n",
            "[3,   320] loss: 0.003640\n",
            "[3,   340] loss: 0.003258\n",
            "[3,   360] loss: 0.003032\n",
            "[3,   380] loss: 0.003134\n",
            "[3,   400] loss: 0.003238\n",
            "[3,   420] loss: 0.003248\n",
            "[3,   440] loss: 0.003022\n",
            "[3,   460] loss: 0.003081\n",
            "Loss of ensemble model 5\n",
            "[1,    20] loss: 0.022698\n",
            "[1,    40] loss: 0.020415\n",
            "[1,    60] loss: 0.016635\n",
            "[1,    80] loss: 0.013908\n",
            "[1,   100] loss: 0.012304\n",
            "[1,   120] loss: 0.011060\n",
            "[1,   140] loss: 0.009471\n",
            "[1,   160] loss: 0.008520\n",
            "[1,   180] loss: 0.008582\n",
            "[1,   200] loss: 0.007657\n",
            "[1,   220] loss: 0.007172\n",
            "[1,   240] loss: 0.006952\n",
            "[1,   260] loss: 0.006886\n",
            "[1,   280] loss: 0.005960\n",
            "[1,   300] loss: 0.006465\n",
            "[1,   320] loss: 0.006211\n",
            "[1,   340] loss: 0.005962\n",
            "[1,   360] loss: 0.005883\n",
            "[1,   380] loss: 0.005499\n",
            "[1,   400] loss: 0.005392\n",
            "[1,   420] loss: 0.005358\n",
            "[1,   440] loss: 0.005076\n",
            "[1,   460] loss: 0.005375\n",
            "[2,    20] loss: 0.006822\n",
            "[2,    40] loss: 0.004853\n",
            "[2,    60] loss: 0.004712\n",
            "[2,    80] loss: 0.004545\n",
            "[2,   100] loss: 0.004486\n",
            "[2,   120] loss: 0.004072\n",
            "[2,   140] loss: 0.004492\n",
            "[2,   160] loss: 0.004578\n",
            "[2,   180] loss: 0.004441\n",
            "[2,   200] loss: 0.004377\n",
            "[2,   220] loss: 0.004262\n",
            "[2,   240] loss: 0.004101\n",
            "[2,   260] loss: 0.004059\n",
            "[2,   280] loss: 0.003820\n",
            "[2,   300] loss: 0.003977\n",
            "[2,   320] loss: 0.004390\n",
            "[2,   340] loss: 0.004122\n",
            "[2,   360] loss: 0.004075\n",
            "[2,   380] loss: 0.003535\n",
            "[2,   400] loss: 0.004237\n",
            "[2,   420] loss: 0.003870\n",
            "[2,   440] loss: 0.003697\n",
            "[2,   460] loss: 0.003834\n",
            "[3,    20] loss: 0.005741\n",
            "[3,    40] loss: 0.003334\n",
            "[3,    60] loss: 0.003731\n",
            "[3,    80] loss: 0.003901\n",
            "[3,   100] loss: 0.003481\n",
            "[3,   120] loss: 0.003416\n",
            "[3,   140] loss: 0.003336\n",
            "[3,   160] loss: 0.003526\n",
            "[3,   180] loss: 0.003429\n",
            "[3,   200] loss: 0.003302\n",
            "[3,   220] loss: 0.003710\n",
            "[3,   240] loss: 0.003466\n",
            "[3,   260] loss: 0.002889\n",
            "[3,   280] loss: 0.003220\n",
            "[3,   300] loss: 0.003356\n",
            "[3,   320] loss: 0.003335\n",
            "[3,   340] loss: 0.003373\n",
            "[3,   360] loss: 0.003315\n",
            "[3,   380] loss: 0.003070\n",
            "[3,   400] loss: 0.003376\n",
            "[3,   420] loss: 0.003509\n",
            "[3,   440] loss: 0.003366\n",
            "[3,   460] loss: 0.003282\n",
            "Loss of ensemble model 6\n",
            "[1,    20] loss: 0.022506\n",
            "[1,    40] loss: 0.019573\n",
            "[1,    60] loss: 0.015621\n",
            "[1,    80] loss: 0.013081\n",
            "[1,   100] loss: 0.012177\n",
            "[1,   120] loss: 0.010576\n",
            "[1,   140] loss: 0.009501\n",
            "[1,   160] loss: 0.008230\n",
            "[1,   180] loss: 0.007753\n",
            "[1,   200] loss: 0.007641\n",
            "[1,   220] loss: 0.007406\n",
            "[1,   240] loss: 0.007038\n",
            "[1,   260] loss: 0.006728\n",
            "[1,   280] loss: 0.006658\n",
            "[1,   300] loss: 0.005919\n",
            "[1,   320] loss: 0.006052\n",
            "[1,   340] loss: 0.005745\n",
            "[1,   360] loss: 0.005864\n",
            "[1,   380] loss: 0.005278\n",
            "[1,   400] loss: 0.005506\n",
            "[1,   420] loss: 0.005545\n",
            "[1,   440] loss: 0.005337\n",
            "[1,   460] loss: 0.005054\n",
            "[2,    20] loss: 0.007095\n",
            "[2,    40] loss: 0.004425\n",
            "[2,    60] loss: 0.004364\n",
            "[2,    80] loss: 0.004551\n",
            "[2,   100] loss: 0.004776\n",
            "[2,   120] loss: 0.004303\n",
            "[2,   140] loss: 0.004568\n",
            "[2,   160] loss: 0.004313\n",
            "[2,   180] loss: 0.003878\n",
            "[2,   200] loss: 0.004265\n",
            "[2,   220] loss: 0.004346\n",
            "[2,   240] loss: 0.004108\n",
            "[2,   260] loss: 0.004125\n",
            "[2,   280] loss: 0.004113\n",
            "[2,   300] loss: 0.003918\n",
            "[2,   320] loss: 0.004073\n",
            "[2,   340] loss: 0.003713\n",
            "[2,   360] loss: 0.003688\n",
            "[2,   380] loss: 0.003668\n",
            "[2,   400] loss: 0.003567\n",
            "[2,   420] loss: 0.003657\n",
            "[2,   440] loss: 0.003668\n",
            "[2,   460] loss: 0.003545\n",
            "[3,    20] loss: 0.005119\n",
            "[3,    40] loss: 0.003807\n",
            "[3,    60] loss: 0.003651\n",
            "[3,    80] loss: 0.003345\n",
            "[3,   100] loss: 0.003308\n",
            "[3,   120] loss: 0.003520\n",
            "[3,   140] loss: 0.003360\n",
            "[3,   160] loss: 0.003305\n",
            "[3,   180] loss: 0.003728\n",
            "[3,   200] loss: 0.003771\n",
            "[3,   220] loss: 0.003508\n",
            "[3,   240] loss: 0.002774\n",
            "[3,   260] loss: 0.003269\n",
            "[3,   280] loss: 0.003279\n",
            "[3,   300] loss: 0.003429\n",
            "[3,   320] loss: 0.003251\n",
            "[3,   340] loss: 0.003398\n",
            "[3,   360] loss: 0.003161\n",
            "[3,   380] loss: 0.003352\n",
            "[3,   400] loss: 0.003117\n",
            "[3,   420] loss: 0.003286\n",
            "[3,   440] loss: 0.003068\n",
            "[3,   460] loss: 0.003289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sxfc4GQAhWK",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Form the encoded sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHMwONS6Ahoy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "82c8c28e-ac71-4ce1-dbf2-28f2c171bc9f"
      },
      "source": [
        "correct = 0\n",
        "train_data = []\n",
        "train_label = []\n",
        "for i in range(ensemble_num):\n",
        "  with torch.no_grad():\n",
        "    for data in trainloader[i]:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs=[]\n",
        "        for i in range(ensemble_num):\n",
        "          outputs.append(model[i](images))\n",
        "          outputs[i]=outputs[i].cpu().numpy()\n",
        "\n",
        "        labels = labels.cpu().numpy()\n",
        "        for i in range(len(outputs[0])):\n",
        "            d = np.concatenate((outputs[0][i], outputs[1][i]), axis=None)\n",
        "            d = np.concatenate((d, outputs[2][i]), axis=None)\n",
        "            d = np.concatenate((d, outputs[3][i]), axis=None)\n",
        "            d = np.concatenate((d, outputs[4][i]), axis=None)\n",
        "            d = np.concatenate((d, outputs[5][i]), axis=None)\n",
        "            d = np.concatenate((d, outputs[6][i]), axis=None)\n",
        "         \n",
        "            train_data.append(d)\n",
        "            train_label.append(labels[i])\n",
        "            \n",
        "print(len(train_label))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "420000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BgrQnHzH1Sp",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: Test correction rate under several Classifers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usoSmNmKH1mo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "dfb25861-423d-421c-b7e7-967378d59d54"
      },
      "source": [
        "#Correction rate under KNeighborsClassifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.05, random_state=0)\n",
        "neigh = KNeighborsClassifier(n_neighbors=10)\n",
        "neigh.fit(X_train, y_train) \n",
        "pred = neigh.predict(X_test)\n",
        "total = 0\n",
        "correct = 1\n",
        "for i in range(len(y_test)):\n",
        "    total += 1\n",
        "    if y_test[i] == pred[i]:\n",
        "        correct += 1\n",
        "print(\"The correction rate under KNN is:\")\n",
        "print(correct * 1.0 / total * 1.0)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The correction rate under KNN is:\n",
            "0.9453333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMy-cF-sKgVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Correction rate under DecisionTreeClassifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.05, random_state=0)\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state=0)\n",
        "tree.fit(X_train, y_train) \n",
        "\n",
        "pred = tree.predict(X_test)\n",
        "\n",
        "total = 0\n",
        "correct = 1\n",
        "for i in range(len(y_test)):\n",
        "    total += 1\n",
        "    if y_test[i] == pred[i]:\n",
        "        correct += 1\n",
        "print(\"The correction rate under Decision Tree is:\")\n",
        "print(correct * 1.0 / total * 1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkQeQJczK3qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Correction rate under RandomForestClassifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.05, random_state=0)\n",
        "\n",
        "forest = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=0)\n",
        "forest.fit(X_train, y_train) \n",
        "\n",
        "pred = forest.predict(X_test)\n",
        "total = 0\n",
        "correct = 1\n",
        "for i in range(len(y_test)):\n",
        "    total += 1\n",
        "    if y_test[i] == pred[i]:\n",
        "        correct += 1\n",
        "print(\"The correction rate under Random Forest is:\")\n",
        "print(correct * 1.0 / total * 1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mm4OVFeLiUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Correction rate under MLPClassifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.05, random_state=0)\n",
        "\n",
        "mlp = MLPClassifier(alpha=1, max_iter=1000)\n",
        "mlp.fit(X_train, y_train) \n",
        "\n",
        "pred = mlp.predict(X_test)\n",
        "total = 0\n",
        "correct = 1\n",
        "for i in range(len(y_test)):\n",
        "    total += 1\n",
        "    if y_test[i] == pred[i]:\n",
        "        correct += 1\n",
        "print(\"The correction rate under MLPClassifier is:\")\n",
        "print(correct * 1.0 / total * 1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}