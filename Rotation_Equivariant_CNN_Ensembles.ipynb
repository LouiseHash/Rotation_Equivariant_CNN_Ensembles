{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rotation_Equivariant_CNN_Ensembles.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2i_HYQtRLUr",
        "colab_type": "text"
      },
      "source": [
        "### Below is the code for 'Rotation-equivariant convolutional neural network ensembles in image processing', published in UbiComp/ISWC '19 Adjunct 2019.\n",
        "\n",
        "Abstract of Essay:\n",
        "For the present engineering of neural networks, rotation invariant is hard to be obtained. Rotation symmetry is an important characteristic in our physical world. In image recognition, using rotated images would largely decrease the performance of neural networks. This situation seriously hindered the application of neural networks in the real-world, such as human tracking, self-driving cars, and intelligent surveillance. In this paper, we would like to present a rotation-equivariant design of convolutional neural network ensembles to counteract the problem of rotated image processing task. This convolutional neural network ensembles combine multiple convolutional neural networks trained by different ranges of rotation angles respectively. In our proposed theory, the model lowers the training difficulty by learning with smaller separations of random rotation angles instead of a huge one. Experiments are reported in this paper. The convolutional neural network ensembles could reach 96.35% on rotated MNIST datasets, 84.9% on rotated Fashion-MNIST datasets, and 91.35% on rotated KMNIST datasets. These results are comparable to current state-of-the-art performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wa_knoiCf_g",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Import useful packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQq2SdEqCWJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models.resnet import ResNet, BasicBlock\n",
        "from torchvision.datasets import MNIST\n",
        "from tqdm.autonotebook import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import inspect\n",
        "import time\n",
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-BsCZFFHAKE",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Load and normalizing the MNIST training datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzCJEVmsHAfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "means = deviations = [0.5]\n",
        "train_transform=[]\n",
        "start_angle=-270\n",
        "end_angle=0\n",
        "rotate_angle=45\n",
        "ensemble_num=(360//rotate_angle)+1\n",
        "for i in range(ensemble_num):\n",
        "  train_transform.append(transforms.Compose([transforms.RandomRotation([start_angle,end_angle]),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize(means, deviations)]))\n",
        "  start_angle=-270+rotate_angle*(i+1)\n",
        "  end_angle=rotate_angle*(i+1)\n",
        "\n",
        "# add trainset \n",
        "trainset=[]\n",
        "for i in range(ensemble_num):\n",
        "  trainset.append(torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform[i]))\n",
        "# add trainloader\n",
        "trainloader=[]\n",
        "for i in range(ensemble_num):\n",
        "    trainloader.append(torch.utils.data.DataLoader(trainset[i], batch_size=128,\n",
        "                                          shuffle=True, num_workers=2))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A1_RXaV3vLc",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Define a Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4in2zxJ23vrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MnistResNet(nn.Module):  \n",
        "    def __init__(self):\n",
        "        super(MnistResNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.drop_out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.drop_out(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU-HThkx4ubE",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Define Loss functions and optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-JJT8oj4utW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# use GPU\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# define models\n",
        "model=[]\n",
        "for i in range(ensemble_num):\n",
        "  model.append(MnistResNet().to(device))\n",
        "  \n",
        "# define optimizers\n",
        "optimizer=[]\n",
        "for i in range(ensemble_num):\n",
        "  optimizer.append(optim.SGD(model[i].parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq7IC55F8c91",
        "colab_type": "text"
      },
      "source": [
        "## Step 5ï¼šTrain each ensemble network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zymEtbj78dUx",
        "colab_type": "code",
        "outputId": "62bfca14-98d7-456c-e0f1-7a9ef8aab612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epoch_range = 6\n",
        "# training loop + eval loop\n",
        "for ensemble_id in range(ensemble_num):\n",
        "    running_loss = 0.0\n",
        "    print(\"Loss of ensemble model\",ensemble_id)\n",
        "    for epoch in range(epoch_range):\n",
        "      for i, data in enumerate(trainloader[ensemble_id], 0):\n",
        "          # get the inputs\n",
        "          inputs, labels = data\n",
        "  #         print(labels.numpy().shape)\n",
        "\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer[ensemble_id].zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = model[ensemble_id](inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer[ensemble_id].step()\n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 20 == 19:    # print every 2000 mini-batches\n",
        "              print('[%d, %5d] loss: %.6f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 2000))\n",
        "              running_loss = 0.0"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss of ensemble model 0\n",
            "[1,    20] loss: 0.022678\n",
            "[1,    40] loss: 0.020412\n",
            "[1,    60] loss: 0.015858\n",
            "[1,    80] loss: 0.013281\n",
            "[1,   100] loss: 0.011597\n",
            "[1,   120] loss: 0.010234\n",
            "[1,   140] loss: 0.008632\n",
            "[1,   160] loss: 0.008028\n",
            "[1,   180] loss: 0.008113\n",
            "[1,   200] loss: 0.007240\n",
            "[1,   220] loss: 0.007296\n",
            "[1,   240] loss: 0.006849\n",
            "[1,   260] loss: 0.006331\n",
            "[1,   280] loss: 0.006468\n",
            "[1,   300] loss: 0.006063\n",
            "[1,   320] loss: 0.005801\n",
            "[1,   340] loss: 0.005738\n",
            "[1,   360] loss: 0.005753\n",
            "[1,   380] loss: 0.005508\n",
            "[1,   400] loss: 0.005297\n",
            "[1,   420] loss: 0.005137\n",
            "[1,   440] loss: 0.004715\n",
            "[1,   460] loss: 0.004737\n",
            "[2,    20] loss: 0.006874\n",
            "[2,    40] loss: 0.004929\n",
            "[2,    60] loss: 0.004289\n",
            "[2,    80] loss: 0.004360\n",
            "[2,   100] loss: 0.004587\n",
            "[2,   120] loss: 0.004260\n",
            "[2,   140] loss: 0.004512\n",
            "[2,   160] loss: 0.004383\n",
            "[2,   180] loss: 0.003911\n",
            "[2,   200] loss: 0.004142\n",
            "[2,   220] loss: 0.003780\n",
            "[2,   240] loss: 0.003992\n",
            "[2,   260] loss: 0.004044\n",
            "[2,   280] loss: 0.003863\n",
            "[2,   300] loss: 0.003578\n",
            "[2,   320] loss: 0.004341\n",
            "[2,   340] loss: 0.003827\n",
            "[2,   360] loss: 0.003563\n",
            "[2,   380] loss: 0.003897\n",
            "[2,   400] loss: 0.003520\n",
            "[2,   420] loss: 0.003464\n",
            "[2,   440] loss: 0.003645\n",
            "[2,   460] loss: 0.003480\n",
            "[3,    20] loss: 0.005187\n",
            "[3,    40] loss: 0.003396\n",
            "[3,    60] loss: 0.003922\n",
            "[3,    80] loss: 0.003453\n",
            "[3,   100] loss: 0.003174\n",
            "[3,   120] loss: 0.003412\n",
            "[3,   140] loss: 0.002965\n",
            "[3,   160] loss: 0.003495\n",
            "[3,   180] loss: 0.003590\n",
            "[3,   200] loss: 0.003235\n",
            "[3,   220] loss: 0.003039\n",
            "[3,   240] loss: 0.003380\n",
            "[3,   260] loss: 0.003337\n",
            "[3,   280] loss: 0.003369\n",
            "[3,   300] loss: 0.003122\n",
            "[3,   320] loss: 0.003224\n",
            "[3,   340] loss: 0.003124\n",
            "[3,   360] loss: 0.003043\n",
            "[3,   380] loss: 0.002966\n",
            "[3,   400] loss: 0.003077\n",
            "[3,   420] loss: 0.003275\n",
            "[3,   440] loss: 0.003101\n",
            "[3,   460] loss: 0.003272\n",
            "[4,    20] loss: 0.004316\n",
            "[4,    40] loss: 0.002955\n",
            "[4,    60] loss: 0.002826\n",
            "[4,    80] loss: 0.003089\n",
            "[4,   100] loss: 0.003017\n",
            "[4,   120] loss: 0.003153\n",
            "[4,   140] loss: 0.002849\n",
            "[4,   160] loss: 0.002827\n",
            "[4,   180] loss: 0.002871\n",
            "[4,   200] loss: 0.002845\n",
            "[4,   220] loss: 0.003088\n",
            "[4,   240] loss: 0.003063\n",
            "[4,   260] loss: 0.002969\n",
            "[4,   280] loss: 0.002809\n",
            "[4,   300] loss: 0.002993\n",
            "[4,   320] loss: 0.003115\n",
            "[4,   340] loss: 0.002846\n",
            "[4,   360] loss: 0.002796\n",
            "[4,   380] loss: 0.003346\n",
            "[4,   400] loss: 0.002988\n",
            "[4,   420] loss: 0.002798\n",
            "[4,   440] loss: 0.002731\n",
            "[4,   460] loss: 0.002738\n",
            "[5,    20] loss: 0.003957\n",
            "[5,    40] loss: 0.002982\n",
            "[5,    60] loss: 0.002814\n",
            "[5,    80] loss: 0.002845\n",
            "[5,   100] loss: 0.002619\n",
            "[5,   120] loss: 0.002716\n",
            "[5,   140] loss: 0.002929\n",
            "[5,   160] loss: 0.002550\n",
            "[5,   180] loss: 0.002916\n",
            "[5,   200] loss: 0.002686\n",
            "[5,   220] loss: 0.002863\n",
            "[5,   240] loss: 0.002744\n",
            "[5,   260] loss: 0.002984\n",
            "[5,   280] loss: 0.002533\n",
            "[5,   300] loss: 0.002516\n",
            "[5,   320] loss: 0.002688\n",
            "[5,   340] loss: 0.002453\n",
            "[5,   360] loss: 0.002878\n",
            "[5,   380] loss: 0.002509\n",
            "[5,   400] loss: 0.002859\n",
            "[5,   420] loss: 0.002479\n",
            "[5,   440] loss: 0.002538\n",
            "[5,   460] loss: 0.002558\n",
            "[6,    20] loss: 0.003806\n",
            "[6,    40] loss: 0.002684\n",
            "[6,    60] loss: 0.002716\n",
            "[6,    80] loss: 0.002394\n",
            "[6,   100] loss: 0.002400\n",
            "[6,   120] loss: 0.002690\n",
            "[6,   140] loss: 0.002424\n",
            "[6,   160] loss: 0.002562\n",
            "[6,   180] loss: 0.002528\n",
            "[6,   200] loss: 0.002491\n",
            "[6,   220] loss: 0.002612\n",
            "[6,   240] loss: 0.002702\n",
            "[6,   260] loss: 0.002725\n",
            "[6,   280] loss: 0.002576\n",
            "[6,   300] loss: 0.002386\n",
            "[6,   320] loss: 0.002589\n",
            "[6,   340] loss: 0.002568\n",
            "[6,   360] loss: 0.002555\n",
            "[6,   380] loss: 0.002532\n",
            "[6,   400] loss: 0.002697\n",
            "[6,   420] loss: 0.002762\n",
            "[6,   440] loss: 0.002412\n",
            "[6,   460] loss: 0.002360\n",
            "Loss of ensemble model 1\n",
            "[1,    20] loss: 0.022589\n",
            "[1,    40] loss: 0.019405\n",
            "[1,    60] loss: 0.015659\n",
            "[1,    80] loss: 0.013065\n",
            "[1,   100] loss: 0.011536\n",
            "[1,   120] loss: 0.010020\n",
            "[1,   140] loss: 0.008931\n",
            "[1,   160] loss: 0.008128\n",
            "[1,   180] loss: 0.008020\n",
            "[1,   200] loss: 0.007640\n",
            "[1,   220] loss: 0.007324\n",
            "[1,   240] loss: 0.006542\n",
            "[1,   260] loss: 0.006403\n",
            "[1,   280] loss: 0.005907\n",
            "[1,   300] loss: 0.006043\n",
            "[1,   320] loss: 0.005595\n",
            "[1,   340] loss: 0.005592\n",
            "[1,   360] loss: 0.005399\n",
            "[1,   380] loss: 0.004960\n",
            "[1,   400] loss: 0.004825\n",
            "[1,   420] loss: 0.005591\n",
            "[1,   440] loss: 0.004733\n",
            "[1,   460] loss: 0.004650\n",
            "[2,    20] loss: 0.007104\n",
            "[2,    40] loss: 0.005004\n",
            "[2,    60] loss: 0.004741\n",
            "[2,    80] loss: 0.004377\n",
            "[2,   100] loss: 0.004466\n",
            "[2,   120] loss: 0.004359\n",
            "[2,   140] loss: 0.004201\n",
            "[2,   160] loss: 0.004112\n",
            "[2,   180] loss: 0.004345\n",
            "[2,   200] loss: 0.004266\n",
            "[2,   220] loss: 0.004275\n",
            "[2,   240] loss: 0.004140\n",
            "[2,   260] loss: 0.003942\n",
            "[2,   280] loss: 0.004093\n",
            "[2,   300] loss: 0.003815\n",
            "[2,   320] loss: 0.003521\n",
            "[2,   340] loss: 0.003985\n",
            "[2,   360] loss: 0.004068\n",
            "[2,   380] loss: 0.003863\n",
            "[2,   400] loss: 0.003477\n",
            "[2,   420] loss: 0.003829\n",
            "[2,   440] loss: 0.003699\n",
            "[2,   460] loss: 0.003730\n",
            "[3,    20] loss: 0.005399\n",
            "[3,    40] loss: 0.003472\n",
            "[3,    60] loss: 0.003530\n",
            "[3,    80] loss: 0.003325\n",
            "[3,   100] loss: 0.003690\n",
            "[3,   120] loss: 0.003509\n",
            "[3,   140] loss: 0.003431\n",
            "[3,   160] loss: 0.003269\n",
            "[3,   180] loss: 0.003547\n",
            "[3,   200] loss: 0.003471\n",
            "[3,   220] loss: 0.003557\n",
            "[3,   240] loss: 0.003378\n",
            "[3,   260] loss: 0.003481\n",
            "[3,   280] loss: 0.003362\n",
            "[3,   300] loss: 0.003066\n",
            "[3,   320] loss: 0.003500\n",
            "[3,   340] loss: 0.003271\n",
            "[3,   360] loss: 0.003435\n",
            "[3,   380] loss: 0.003326\n",
            "[3,   400] loss: 0.003069\n",
            "[3,   420] loss: 0.003190\n",
            "[3,   440] loss: 0.003259\n",
            "[3,   460] loss: 0.003178\n",
            "[4,    20] loss: 0.004849\n",
            "[4,    40] loss: 0.002859\n",
            "[4,    60] loss: 0.003380\n",
            "[4,    80] loss: 0.003018\n",
            "[4,   100] loss: 0.003143\n",
            "[4,   120] loss: 0.002939\n",
            "[4,   140] loss: 0.003186\n",
            "[4,   160] loss: 0.003177\n",
            "[4,   180] loss: 0.002905\n",
            "[4,   200] loss: 0.003083\n",
            "[4,   220] loss: 0.002695\n",
            "[4,   240] loss: 0.003000\n",
            "[4,   260] loss: 0.003063\n",
            "[4,   280] loss: 0.003058\n",
            "[4,   300] loss: 0.003076\n",
            "[4,   320] loss: 0.003003\n",
            "[4,   340] loss: 0.002879\n",
            "[4,   360] loss: 0.003053\n",
            "[4,   380] loss: 0.003054\n",
            "[4,   400] loss: 0.002912\n",
            "[4,   420] loss: 0.003112\n",
            "[4,   440] loss: 0.002787\n",
            "[4,   460] loss: 0.002832\n",
            "[5,    20] loss: 0.004250\n",
            "[5,    40] loss: 0.002864\n",
            "[5,    60] loss: 0.002543\n",
            "[5,    80] loss: 0.002717\n",
            "[5,   100] loss: 0.002762\n",
            "[5,   120] loss: 0.002841\n",
            "[5,   140] loss: 0.002926\n",
            "[5,   160] loss: 0.002928\n",
            "[5,   180] loss: 0.002795\n",
            "[5,   200] loss: 0.002574\n",
            "[5,   220] loss: 0.002841\n",
            "[5,   240] loss: 0.002543\n",
            "[5,   260] loss: 0.002718\n",
            "[5,   280] loss: 0.002933\n",
            "[5,   300] loss: 0.002850\n",
            "[5,   320] loss: 0.002873\n",
            "[5,   340] loss: 0.002581\n",
            "[5,   360] loss: 0.002825\n",
            "[5,   380] loss: 0.002744\n",
            "[5,   400] loss: 0.002695\n",
            "[5,   420] loss: 0.002711\n",
            "[5,   440] loss: 0.002483\n",
            "[5,   460] loss: 0.002776\n",
            "[6,    20] loss: 0.003460\n",
            "[6,    40] loss: 0.002429\n",
            "[6,    60] loss: 0.002831\n",
            "[6,    80] loss: 0.002464\n",
            "[6,   100] loss: 0.002742\n",
            "[6,   120] loss: 0.002546\n",
            "[6,   140] loss: 0.002988\n",
            "[6,   160] loss: 0.002828\n",
            "[6,   180] loss: 0.002685\n",
            "[6,   200] loss: 0.002466\n",
            "[6,   220] loss: 0.002799\n",
            "[6,   240] loss: 0.002648\n",
            "[6,   260] loss: 0.002532\n",
            "[6,   280] loss: 0.002855\n",
            "[6,   300] loss: 0.002695\n",
            "[6,   320] loss: 0.002649\n",
            "[6,   340] loss: 0.002722\n",
            "[6,   360] loss: 0.002805\n",
            "[6,   380] loss: 0.002641\n",
            "[6,   400] loss: 0.002408\n",
            "[6,   420] loss: 0.002636\n",
            "[6,   440] loss: 0.002578\n",
            "[6,   460] loss: 0.002610\n",
            "Loss of ensemble model 2\n",
            "[1,    20] loss: 0.022679\n",
            "[1,    40] loss: 0.020278\n",
            "[1,    60] loss: 0.016333\n",
            "[1,    80] loss: 0.013459\n",
            "[1,   100] loss: 0.012135\n",
            "[1,   120] loss: 0.009725\n",
            "[1,   140] loss: 0.009448\n",
            "[1,   160] loss: 0.008657\n",
            "[1,   180] loss: 0.008161\n",
            "[1,   200] loss: 0.007259\n",
            "[1,   220] loss: 0.007421\n",
            "[1,   240] loss: 0.007198\n",
            "[1,   260] loss: 0.006671\n",
            "[1,   280] loss: 0.006088\n",
            "[1,   300] loss: 0.006180\n",
            "[1,   320] loss: 0.006056\n",
            "[1,   340] loss: 0.005664\n",
            "[1,   360] loss: 0.005806\n",
            "[1,   380] loss: 0.005521\n",
            "[1,   400] loss: 0.005245\n",
            "[1,   420] loss: 0.004949\n",
            "[1,   440] loss: 0.005263\n",
            "[1,   460] loss: 0.005086\n",
            "[2,    20] loss: 0.007082\n",
            "[2,    40] loss: 0.005115\n",
            "[2,    60] loss: 0.004876\n",
            "[2,    80] loss: 0.004466\n",
            "[2,   100] loss: 0.004553\n",
            "[2,   120] loss: 0.004620\n",
            "[2,   140] loss: 0.004531\n",
            "[2,   160] loss: 0.004344\n",
            "[2,   180] loss: 0.004421\n",
            "[2,   200] loss: 0.004446\n",
            "[2,   220] loss: 0.004215\n",
            "[2,   240] loss: 0.004070\n",
            "[2,   260] loss: 0.004291\n",
            "[2,   280] loss: 0.004011\n",
            "[2,   300] loss: 0.004067\n",
            "[2,   320] loss: 0.003924\n",
            "[2,   340] loss: 0.003999\n",
            "[2,   360] loss: 0.003934\n",
            "[2,   380] loss: 0.003769\n",
            "[2,   400] loss: 0.004367\n",
            "[2,   420] loss: 0.003796\n",
            "[2,   440] loss: 0.003826\n",
            "[2,   460] loss: 0.003594\n",
            "[3,    20] loss: 0.005014\n",
            "[3,    40] loss: 0.003589\n",
            "[3,    60] loss: 0.003491\n",
            "[3,    80] loss: 0.003361\n",
            "[3,   100] loss: 0.003471\n",
            "[3,   120] loss: 0.003649\n",
            "[3,   140] loss: 0.003426\n",
            "[3,   160] loss: 0.003717\n",
            "[3,   180] loss: 0.003321\n",
            "[3,   200] loss: 0.003123\n",
            "[3,   220] loss: 0.003730\n",
            "[3,   240] loss: 0.003197\n",
            "[3,   260] loss: 0.003190\n",
            "[3,   280] loss: 0.003493\n",
            "[3,   300] loss: 0.003189\n",
            "[3,   320] loss: 0.003140\n",
            "[3,   340] loss: 0.003611\n",
            "[3,   360] loss: 0.003392\n",
            "[3,   380] loss: 0.003173\n",
            "[3,   400] loss: 0.003307\n",
            "[3,   420] loss: 0.003374\n",
            "[3,   440] loss: 0.003238\n",
            "[3,   460] loss: 0.002979\n",
            "[4,    20] loss: 0.004761\n",
            "[4,    40] loss: 0.003267\n",
            "[4,    60] loss: 0.003091\n",
            "[4,    80] loss: 0.003256\n",
            "[4,   100] loss: 0.002965\n",
            "[4,   120] loss: 0.003371\n",
            "[4,   140] loss: 0.003170\n",
            "[4,   160] loss: 0.003208\n",
            "[4,   180] loss: 0.002938\n",
            "[4,   200] loss: 0.002987\n",
            "[4,   220] loss: 0.003074\n",
            "[4,   240] loss: 0.002962\n",
            "[4,   260] loss: 0.002853\n",
            "[4,   280] loss: 0.003391\n",
            "[4,   300] loss: 0.002893\n",
            "[4,   320] loss: 0.003147\n",
            "[4,   340] loss: 0.003100\n",
            "[4,   360] loss: 0.003036\n",
            "[4,   380] loss: 0.002769\n",
            "[4,   400] loss: 0.002688\n",
            "[4,   420] loss: 0.002801\n",
            "[4,   440] loss: 0.003006\n",
            "[4,   460] loss: 0.002739\n",
            "[5,    20] loss: 0.003874\n",
            "[5,    40] loss: 0.002944\n",
            "[5,    60] loss: 0.002886\n",
            "[5,    80] loss: 0.002849\n",
            "[5,   100] loss: 0.002923\n",
            "[5,   120] loss: 0.002911\n",
            "[5,   140] loss: 0.002774\n",
            "[5,   160] loss: 0.002881\n",
            "[5,   180] loss: 0.002629\n",
            "[5,   200] loss: 0.002636\n",
            "[5,   220] loss: 0.003056\n",
            "[5,   240] loss: 0.002659\n",
            "[5,   260] loss: 0.002508\n",
            "[5,   280] loss: 0.002784\n",
            "[5,   300] loss: 0.002637\n",
            "[5,   320] loss: 0.002918\n",
            "[5,   340] loss: 0.002923\n",
            "[5,   360] loss: 0.002532\n",
            "[5,   380] loss: 0.002726\n",
            "[5,   400] loss: 0.002581\n",
            "[5,   420] loss: 0.002940\n",
            "[5,   440] loss: 0.002909\n",
            "[5,   460] loss: 0.002638\n",
            "[6,    20] loss: 0.003822\n",
            "[6,    40] loss: 0.002818\n",
            "[6,    60] loss: 0.002724\n",
            "[6,    80] loss: 0.002756\n",
            "[6,   100] loss: 0.002609\n",
            "[6,   120] loss: 0.002687\n",
            "[6,   140] loss: 0.002537\n",
            "[6,   160] loss: 0.002527\n",
            "[6,   180] loss: 0.003038\n",
            "[6,   200] loss: 0.002604\n",
            "[6,   220] loss: 0.002743\n",
            "[6,   240] loss: 0.002465\n",
            "[6,   260] loss: 0.002494\n",
            "[6,   280] loss: 0.002730\n",
            "[6,   300] loss: 0.002372\n",
            "[6,   320] loss: 0.002235\n",
            "[6,   340] loss: 0.002323\n",
            "[6,   360] loss: 0.002427\n",
            "[6,   380] loss: 0.002730\n",
            "[6,   400] loss: 0.002522\n",
            "[6,   420] loss: 0.002774\n",
            "[6,   440] loss: 0.002506\n",
            "[6,   460] loss: 0.002505\n",
            "Loss of ensemble model 3\n",
            "[1,    20] loss: 0.022760\n",
            "[1,    40] loss: 0.020668\n",
            "[1,    60] loss: 0.016288\n",
            "[1,    80] loss: 0.013636\n",
            "[1,   100] loss: 0.012150\n",
            "[1,   120] loss: 0.010458\n",
            "[1,   140] loss: 0.009081\n",
            "[1,   160] loss: 0.008347\n",
            "[1,   180] loss: 0.007697\n",
            "[1,   200] loss: 0.007300\n",
            "[1,   220] loss: 0.007130\n",
            "[1,   240] loss: 0.006946\n",
            "[1,   260] loss: 0.006416\n",
            "[1,   280] loss: 0.006301\n",
            "[1,   300] loss: 0.005785\n",
            "[1,   320] loss: 0.006161\n",
            "[1,   340] loss: 0.006012\n",
            "[1,   360] loss: 0.005385\n",
            "[1,   380] loss: 0.005490\n",
            "[1,   400] loss: 0.005399\n",
            "[1,   420] loss: 0.005441\n",
            "[1,   440] loss: 0.005101\n",
            "[1,   460] loss: 0.004758\n",
            "[2,    20] loss: 0.006902\n",
            "[2,    40] loss: 0.004768\n",
            "[2,    60] loss: 0.004433\n",
            "[2,    80] loss: 0.004464\n",
            "[2,   100] loss: 0.004326\n",
            "[2,   120] loss: 0.004148\n",
            "[2,   140] loss: 0.004661\n",
            "[2,   160] loss: 0.004443\n",
            "[2,   180] loss: 0.003868\n",
            "[2,   200] loss: 0.004149\n",
            "[2,   220] loss: 0.004459\n",
            "[2,   240] loss: 0.003938\n",
            "[2,   260] loss: 0.003859\n",
            "[2,   280] loss: 0.003936\n",
            "[2,   300] loss: 0.004227\n",
            "[2,   320] loss: 0.003731\n",
            "[2,   340] loss: 0.003844\n",
            "[2,   360] loss: 0.003695\n",
            "[2,   380] loss: 0.003969\n",
            "[2,   400] loss: 0.003653\n",
            "[2,   420] loss: 0.003540\n",
            "[2,   440] loss: 0.003649\n",
            "[2,   460] loss: 0.003671\n",
            "[3,    20] loss: 0.005685\n",
            "[3,    40] loss: 0.003567\n",
            "[3,    60] loss: 0.003307\n",
            "[3,    80] loss: 0.003592\n",
            "[3,   100] loss: 0.003369\n",
            "[3,   120] loss: 0.003716\n",
            "[3,   140] loss: 0.003375\n",
            "[3,   160] loss: 0.003741\n",
            "[3,   180] loss: 0.003613\n",
            "[3,   200] loss: 0.003084\n",
            "[3,   220] loss: 0.003333\n",
            "[3,   240] loss: 0.003297\n",
            "[3,   260] loss: 0.003570\n",
            "[3,   280] loss: 0.003020\n",
            "[3,   300] loss: 0.003256\n",
            "[3,   320] loss: 0.003329\n",
            "[3,   340] loss: 0.003303\n",
            "[3,   360] loss: 0.003027\n",
            "[3,   380] loss: 0.003276\n",
            "[3,   400] loss: 0.003450\n",
            "[3,   420] loss: 0.003347\n",
            "[3,   440] loss: 0.003083\n",
            "[3,   460] loss: 0.003256\n",
            "[4,    20] loss: 0.004524\n",
            "[4,    40] loss: 0.002964\n",
            "[4,    60] loss: 0.003199\n",
            "[4,    80] loss: 0.002790\n",
            "[4,   100] loss: 0.002958\n",
            "[4,   120] loss: 0.003177\n",
            "[4,   140] loss: 0.003251\n",
            "[4,   160] loss: 0.003066\n",
            "[4,   180] loss: 0.002945\n",
            "[4,   200] loss: 0.003195\n",
            "[4,   220] loss: 0.002974\n",
            "[4,   240] loss: 0.002722\n",
            "[4,   260] loss: 0.002670\n",
            "[4,   280] loss: 0.002763\n",
            "[4,   300] loss: 0.003075\n",
            "[4,   320] loss: 0.003017\n",
            "[4,   340] loss: 0.003110\n",
            "[4,   360] loss: 0.002791\n",
            "[4,   380] loss: 0.003018\n",
            "[4,   400] loss: 0.002856\n",
            "[4,   420] loss: 0.002923\n",
            "[4,   440] loss: 0.003012\n",
            "[4,   460] loss: 0.002891\n",
            "[5,    20] loss: 0.004120\n",
            "[5,    40] loss: 0.002684\n",
            "[5,    60] loss: 0.002632\n",
            "[5,    80] loss: 0.002752\n",
            "[5,   100] loss: 0.002836\n",
            "[5,   120] loss: 0.002634\n",
            "[5,   140] loss: 0.002676\n",
            "[5,   160] loss: 0.002811\n",
            "[5,   180] loss: 0.002748\n",
            "[5,   200] loss: 0.002765\n",
            "[5,   220] loss: 0.002681\n",
            "[5,   240] loss: 0.002761\n",
            "[5,   260] loss: 0.002833\n",
            "[5,   280] loss: 0.002916\n",
            "[5,   300] loss: 0.002948\n",
            "[5,   320] loss: 0.002779\n",
            "[5,   340] loss: 0.002856\n",
            "[5,   360] loss: 0.002656\n",
            "[5,   380] loss: 0.002623\n",
            "[5,   400] loss: 0.002531\n",
            "[5,   420] loss: 0.002605\n",
            "[5,   440] loss: 0.002969\n",
            "[5,   460] loss: 0.002729\n",
            "[6,    20] loss: 0.003427\n",
            "[6,    40] loss: 0.002763\n",
            "[6,    60] loss: 0.002689\n",
            "[6,    80] loss: 0.002667\n",
            "[6,   100] loss: 0.002341\n",
            "[6,   120] loss: 0.002784\n",
            "[6,   140] loss: 0.002562\n",
            "[6,   160] loss: 0.002572\n",
            "[6,   180] loss: 0.002629\n",
            "[6,   200] loss: 0.002476\n",
            "[6,   220] loss: 0.002294\n",
            "[6,   240] loss: 0.002622\n",
            "[6,   260] loss: 0.002705\n",
            "[6,   280] loss: 0.002654\n",
            "[6,   300] loss: 0.002435\n",
            "[6,   320] loss: 0.002841\n",
            "[6,   340] loss: 0.002560\n",
            "[6,   360] loss: 0.002389\n",
            "[6,   380] loss: 0.002438\n",
            "[6,   400] loss: 0.002517\n",
            "[6,   420] loss: 0.002454\n",
            "[6,   440] loss: 0.002450\n",
            "[6,   460] loss: 0.002350\n",
            "Loss of ensemble model 4\n",
            "[1,    20] loss: 0.022700\n",
            "[1,    40] loss: 0.019926\n",
            "[1,    60] loss: 0.015926\n",
            "[1,    80] loss: 0.013470\n",
            "[1,   100] loss: 0.011661\n",
            "[1,   120] loss: 0.010328\n",
            "[1,   140] loss: 0.009245\n",
            "[1,   160] loss: 0.008531\n",
            "[1,   180] loss: 0.008353\n",
            "[1,   200] loss: 0.007908\n",
            "[1,   220] loss: 0.007320\n",
            "[1,   240] loss: 0.007099\n",
            "[1,   260] loss: 0.006514\n",
            "[1,   280] loss: 0.006116\n",
            "[1,   300] loss: 0.005949\n",
            "[1,   320] loss: 0.005725\n",
            "[1,   340] loss: 0.005930\n",
            "[1,   360] loss: 0.005255\n",
            "[1,   380] loss: 0.005541\n",
            "[1,   400] loss: 0.005578\n",
            "[1,   420] loss: 0.005152\n",
            "[1,   440] loss: 0.004914\n",
            "[1,   460] loss: 0.005016\n",
            "[2,    20] loss: 0.006904\n",
            "[2,    40] loss: 0.004707\n",
            "[2,    60] loss: 0.004732\n",
            "[2,    80] loss: 0.004451\n",
            "[2,   100] loss: 0.004130\n",
            "[2,   120] loss: 0.004389\n",
            "[2,   140] loss: 0.004263\n",
            "[2,   160] loss: 0.004395\n",
            "[2,   180] loss: 0.004327\n",
            "[2,   200] loss: 0.004216\n",
            "[2,   220] loss: 0.003936\n",
            "[2,   240] loss: 0.003830\n",
            "[2,   260] loss: 0.004009\n",
            "[2,   280] loss: 0.003742\n",
            "[2,   300] loss: 0.003862\n",
            "[2,   320] loss: 0.003495\n",
            "[2,   340] loss: 0.003783\n",
            "[2,   360] loss: 0.004280\n",
            "[2,   380] loss: 0.003865\n",
            "[2,   400] loss: 0.004058\n",
            "[2,   420] loss: 0.003505\n",
            "[2,   440] loss: 0.003580\n",
            "[2,   460] loss: 0.003819\n",
            "[3,    20] loss: 0.005299\n",
            "[3,    40] loss: 0.003624\n",
            "[3,    60] loss: 0.003513\n",
            "[3,    80] loss: 0.003321\n",
            "[3,   100] loss: 0.003664\n",
            "[3,   120] loss: 0.003584\n",
            "[3,   140] loss: 0.003319\n",
            "[3,   160] loss: 0.003580\n",
            "[3,   180] loss: 0.003563\n",
            "[3,   200] loss: 0.003495\n",
            "[3,   220] loss: 0.003575\n",
            "[3,   240] loss: 0.003436\n",
            "[3,   260] loss: 0.003177\n",
            "[3,   280] loss: 0.003066\n",
            "[3,   300] loss: 0.003298\n",
            "[3,   320] loss: 0.003374\n",
            "[3,   340] loss: 0.003424\n",
            "[3,   360] loss: 0.003228\n",
            "[3,   380] loss: 0.003175\n",
            "[3,   400] loss: 0.003281\n",
            "[3,   420] loss: 0.002992\n",
            "[3,   440] loss: 0.002937\n",
            "[3,   460] loss: 0.003218\n",
            "[4,    20] loss: 0.004528\n",
            "[4,    40] loss: 0.002965\n",
            "[4,    60] loss: 0.003382\n",
            "[4,    80] loss: 0.003068\n",
            "[4,   100] loss: 0.003084\n",
            "[4,   120] loss: 0.002942\n",
            "[4,   140] loss: 0.003140\n",
            "[4,   160] loss: 0.002940\n",
            "[4,   180] loss: 0.002787\n",
            "[4,   200] loss: 0.002946\n",
            "[4,   220] loss: 0.003236\n",
            "[4,   240] loss: 0.003182\n",
            "[4,   260] loss: 0.002936\n",
            "[4,   280] loss: 0.003209\n",
            "[4,   300] loss: 0.002824\n",
            "[4,   320] loss: 0.002871\n",
            "[4,   340] loss: 0.003020\n",
            "[4,   360] loss: 0.003263\n",
            "[4,   380] loss: 0.002643\n",
            "[4,   400] loss: 0.002792\n",
            "[4,   420] loss: 0.002878\n",
            "[4,   440] loss: 0.002687\n",
            "[4,   460] loss: 0.002877\n",
            "[5,    20] loss: 0.004246\n",
            "[5,    40] loss: 0.002810\n",
            "[5,    60] loss: 0.003034\n",
            "[5,    80] loss: 0.002877\n",
            "[5,   100] loss: 0.002736\n",
            "[5,   120] loss: 0.002708\n",
            "[5,   140] loss: 0.002997\n",
            "[5,   160] loss: 0.002809\n",
            "[5,   180] loss: 0.002914\n",
            "[5,   200] loss: 0.002763\n",
            "[5,   220] loss: 0.002907\n",
            "[5,   240] loss: 0.002224\n",
            "[5,   260] loss: 0.002839\n",
            "[5,   280] loss: 0.002686\n",
            "[5,   300] loss: 0.002391\n",
            "[5,   320] loss: 0.002730\n",
            "[5,   340] loss: 0.002885\n",
            "[5,   360] loss: 0.002737\n",
            "[5,   380] loss: 0.002692\n",
            "[5,   400] loss: 0.002441\n",
            "[5,   420] loss: 0.002567\n",
            "[5,   440] loss: 0.002854\n",
            "[5,   460] loss: 0.002916\n",
            "[6,    20] loss: 0.003913\n",
            "[6,    40] loss: 0.002521\n",
            "[6,    60] loss: 0.002595\n",
            "[6,    80] loss: 0.002486\n",
            "[6,   100] loss: 0.002455\n",
            "[6,   120] loss: 0.002651\n",
            "[6,   140] loss: 0.002485\n",
            "[6,   160] loss: 0.002686\n",
            "[6,   180] loss: 0.002829\n",
            "[6,   200] loss: 0.002824\n",
            "[6,   220] loss: 0.002495\n",
            "[6,   240] loss: 0.002554\n",
            "[6,   260] loss: 0.002612\n",
            "[6,   280] loss: 0.002700\n",
            "[6,   300] loss: 0.002578\n",
            "[6,   320] loss: 0.002666\n",
            "[6,   340] loss: 0.002351\n",
            "[6,   360] loss: 0.002542\n",
            "[6,   380] loss: 0.002579\n",
            "[6,   400] loss: 0.002680\n",
            "[6,   420] loss: 0.002476\n",
            "[6,   440] loss: 0.002753\n",
            "[6,   460] loss: 0.002390\n",
            "Loss of ensemble model 5\n",
            "[1,    20] loss: 0.022679\n",
            "[1,    40] loss: 0.019822\n",
            "[1,    60] loss: 0.015880\n",
            "[1,    80] loss: 0.012971\n",
            "[1,   100] loss: 0.011214\n",
            "[1,   120] loss: 0.010111\n",
            "[1,   140] loss: 0.008548\n",
            "[1,   160] loss: 0.008472\n",
            "[1,   180] loss: 0.007436\n",
            "[1,   200] loss: 0.007193\n",
            "[1,   220] loss: 0.007269\n",
            "[1,   240] loss: 0.006824\n",
            "[1,   260] loss: 0.006277\n",
            "[1,   280] loss: 0.006319\n",
            "[1,   300] loss: 0.005855\n",
            "[1,   320] loss: 0.006034\n",
            "[1,   340] loss: 0.005372\n",
            "[1,   360] loss: 0.005623\n",
            "[1,   380] loss: 0.005295\n",
            "[1,   400] loss: 0.005426\n",
            "[1,   420] loss: 0.005004\n",
            "[1,   440] loss: 0.004999\n",
            "[1,   460] loss: 0.004901\n",
            "[2,    20] loss: 0.006439\n",
            "[2,    40] loss: 0.004948\n",
            "[2,    60] loss: 0.004674\n",
            "[2,    80] loss: 0.004577\n",
            "[2,   100] loss: 0.004376\n",
            "[2,   120] loss: 0.004261\n",
            "[2,   140] loss: 0.004268\n",
            "[2,   160] loss: 0.004149\n",
            "[2,   180] loss: 0.004346\n",
            "[2,   200] loss: 0.003847\n",
            "[2,   220] loss: 0.003907\n",
            "[2,   240] loss: 0.003723\n",
            "[2,   260] loss: 0.004205\n",
            "[2,   280] loss: 0.003806\n",
            "[2,   300] loss: 0.003701\n",
            "[2,   320] loss: 0.003988\n",
            "[2,   340] loss: 0.004039\n",
            "[2,   360] loss: 0.003986\n",
            "[2,   380] loss: 0.003715\n",
            "[2,   400] loss: 0.003839\n",
            "[2,   420] loss: 0.003312\n",
            "[2,   440] loss: 0.003636\n",
            "[2,   460] loss: 0.003700\n",
            "[3,    20] loss: 0.005676\n",
            "[3,    40] loss: 0.003555\n",
            "[3,    60] loss: 0.003190\n",
            "[3,    80] loss: 0.003788\n",
            "[3,   100] loss: 0.003530\n",
            "[3,   120] loss: 0.003478\n",
            "[3,   140] loss: 0.003474\n",
            "[3,   160] loss: 0.003386\n",
            "[3,   180] loss: 0.003608\n",
            "[3,   200] loss: 0.003205\n",
            "[3,   220] loss: 0.003504\n",
            "[3,   240] loss: 0.003325\n",
            "[3,   260] loss: 0.003395\n",
            "[3,   280] loss: 0.003349\n",
            "[3,   300] loss: 0.003618\n",
            "[3,   320] loss: 0.003355\n",
            "[3,   340] loss: 0.003391\n",
            "[3,   360] loss: 0.003102\n",
            "[3,   380] loss: 0.002960\n",
            "[3,   400] loss: 0.003139\n",
            "[3,   420] loss: 0.003087\n",
            "[3,   440] loss: 0.002683\n",
            "[3,   460] loss: 0.003657\n",
            "[4,    20] loss: 0.004414\n",
            "[4,    40] loss: 0.003288\n",
            "[4,    60] loss: 0.002835\n",
            "[4,    80] loss: 0.003144\n",
            "[4,   100] loss: 0.002680\n",
            "[4,   120] loss: 0.003010\n",
            "[4,   140] loss: 0.003043\n",
            "[4,   160] loss: 0.002966\n",
            "[4,   180] loss: 0.002767\n",
            "[4,   200] loss: 0.003352\n",
            "[4,   220] loss: 0.002981\n",
            "[4,   240] loss: 0.003094\n",
            "[4,   260] loss: 0.003097\n",
            "[4,   280] loss: 0.003043\n",
            "[4,   300] loss: 0.002599\n",
            "[4,   320] loss: 0.002924\n",
            "[4,   340] loss: 0.002938\n",
            "[4,   360] loss: 0.002920\n",
            "[4,   380] loss: 0.002929\n",
            "[4,   400] loss: 0.002774\n",
            "[4,   420] loss: 0.003054\n",
            "[4,   440] loss: 0.002767\n",
            "[4,   460] loss: 0.002963\n",
            "[5,    20] loss: 0.004063\n",
            "[5,    40] loss: 0.002954\n",
            "[5,    60] loss: 0.002838\n",
            "[5,    80] loss: 0.002681\n",
            "[5,   100] loss: 0.002856\n",
            "[5,   120] loss: 0.002785\n",
            "[5,   140] loss: 0.002936\n",
            "[5,   160] loss: 0.002784\n",
            "[5,   180] loss: 0.002923\n",
            "[5,   200] loss: 0.002823\n",
            "[5,   220] loss: 0.002778\n",
            "[5,   240] loss: 0.002815\n",
            "[5,   260] loss: 0.002920\n",
            "[5,   280] loss: 0.002913\n",
            "[5,   300] loss: 0.002679\n",
            "[5,   320] loss: 0.002723\n",
            "[5,   340] loss: 0.002924\n",
            "[5,   360] loss: 0.002736\n",
            "[5,   380] loss: 0.002528\n",
            "[5,   400] loss: 0.002787\n",
            "[5,   420] loss: 0.002823\n",
            "[5,   440] loss: 0.002152\n",
            "[5,   460] loss: 0.002381\n",
            "[6,    20] loss: 0.003730\n",
            "[6,    40] loss: 0.002793\n",
            "[6,    60] loss: 0.002713\n",
            "[6,    80] loss: 0.002750\n",
            "[6,   100] loss: 0.002476\n",
            "[6,   120] loss: 0.002761\n",
            "[6,   140] loss: 0.002512\n",
            "[6,   160] loss: 0.002538\n",
            "[6,   180] loss: 0.002652\n",
            "[6,   200] loss: 0.002499\n",
            "[6,   220] loss: 0.002534\n",
            "[6,   240] loss: 0.002562\n",
            "[6,   260] loss: 0.002386\n",
            "[6,   280] loss: 0.002796\n",
            "[6,   300] loss: 0.002760\n",
            "[6,   320] loss: 0.002618\n",
            "[6,   340] loss: 0.002497\n",
            "[6,   360] loss: 0.002623\n",
            "[6,   380] loss: 0.002528\n",
            "[6,   400] loss: 0.002640\n",
            "[6,   420] loss: 0.002448\n",
            "[6,   440] loss: 0.002378\n",
            "[6,   460] loss: 0.002435\n",
            "Loss of ensemble model 6\n",
            "[1,    20] loss: 0.022662\n",
            "[1,    40] loss: 0.020172\n",
            "[1,    60] loss: 0.015905\n",
            "[1,    80] loss: 0.013342\n",
            "[1,   100] loss: 0.011338\n",
            "[1,   120] loss: 0.010311\n",
            "[1,   140] loss: 0.009175\n",
            "[1,   160] loss: 0.008040\n",
            "[1,   180] loss: 0.007730\n",
            "[1,   200] loss: 0.007437\n",
            "[1,   220] loss: 0.006817\n",
            "[1,   240] loss: 0.006633\n",
            "[1,   260] loss: 0.006749\n",
            "[1,   280] loss: 0.006047\n",
            "[1,   300] loss: 0.006054\n",
            "[1,   320] loss: 0.005698\n",
            "[1,   340] loss: 0.006298\n",
            "[1,   360] loss: 0.005677\n",
            "[1,   380] loss: 0.005115\n",
            "[1,   400] loss: 0.005097\n",
            "[1,   420] loss: 0.005076\n",
            "[1,   440] loss: 0.004956\n",
            "[1,   460] loss: 0.005058\n",
            "[2,    20] loss: 0.006613\n",
            "[2,    40] loss: 0.004722\n",
            "[2,    60] loss: 0.004419\n",
            "[2,    80] loss: 0.004414\n",
            "[2,   100] loss: 0.004806\n",
            "[2,   120] loss: 0.004524\n",
            "[2,   140] loss: 0.004482\n",
            "[2,   160] loss: 0.004201\n",
            "[2,   180] loss: 0.004172\n",
            "[2,   200] loss: 0.004031\n",
            "[2,   220] loss: 0.003811\n",
            "[2,   240] loss: 0.004101\n",
            "[2,   260] loss: 0.004021\n",
            "[2,   280] loss: 0.004060\n",
            "[2,   300] loss: 0.003799\n",
            "[2,   320] loss: 0.003699\n",
            "[2,   340] loss: 0.003739\n",
            "[2,   360] loss: 0.003768\n",
            "[2,   380] loss: 0.003744\n",
            "[2,   400] loss: 0.003906\n",
            "[2,   420] loss: 0.003359\n",
            "[2,   440] loss: 0.003573\n",
            "[2,   460] loss: 0.003489\n",
            "[3,    20] loss: 0.005553\n",
            "[3,    40] loss: 0.003499\n",
            "[3,    60] loss: 0.003189\n",
            "[3,    80] loss: 0.003193\n",
            "[3,   100] loss: 0.003190\n",
            "[3,   120] loss: 0.003367\n",
            "[3,   140] loss: 0.003240\n",
            "[3,   160] loss: 0.003206\n",
            "[3,   180] loss: 0.003454\n",
            "[3,   200] loss: 0.003776\n",
            "[3,   220] loss: 0.003313\n",
            "[3,   240] loss: 0.003328\n",
            "[3,   260] loss: 0.003377\n",
            "[3,   280] loss: 0.003380\n",
            "[3,   300] loss: 0.003272\n",
            "[3,   320] loss: 0.003627\n",
            "[3,   340] loss: 0.003424\n",
            "[3,   360] loss: 0.003265\n",
            "[3,   380] loss: 0.003209\n",
            "[3,   400] loss: 0.003252\n",
            "[3,   420] loss: 0.003206\n",
            "[3,   440] loss: 0.003585\n",
            "[3,   460] loss: 0.003163\n",
            "[4,    20] loss: 0.004368\n",
            "[4,    40] loss: 0.003085\n",
            "[4,    60] loss: 0.003129\n",
            "[4,    80] loss: 0.002987\n",
            "[4,   100] loss: 0.002957\n",
            "[4,   120] loss: 0.002865\n",
            "[4,   140] loss: 0.003344\n",
            "[4,   160] loss: 0.003139\n",
            "[4,   180] loss: 0.002960\n",
            "[4,   200] loss: 0.003080\n",
            "[4,   220] loss: 0.002991\n",
            "[4,   240] loss: 0.003062\n",
            "[4,   260] loss: 0.003247\n",
            "[4,   280] loss: 0.003380\n",
            "[4,   300] loss: 0.002655\n",
            "[4,   320] loss: 0.002563\n",
            "[4,   340] loss: 0.002833\n",
            "[4,   360] loss: 0.002890\n",
            "[4,   380] loss: 0.003089\n",
            "[4,   400] loss: 0.002757\n",
            "[4,   420] loss: 0.002854\n",
            "[4,   440] loss: 0.002885\n",
            "[4,   460] loss: 0.002666\n",
            "[5,    20] loss: 0.004178\n",
            "[5,    40] loss: 0.002610\n",
            "[5,    60] loss: 0.002902\n",
            "[5,    80] loss: 0.002672\n",
            "[5,   100] loss: 0.002762\n",
            "[5,   120] loss: 0.002487\n",
            "[5,   140] loss: 0.002850\n",
            "[5,   160] loss: 0.002851\n",
            "[5,   180] loss: 0.002546\n",
            "[5,   200] loss: 0.002821\n",
            "[5,   220] loss: 0.002908\n",
            "[5,   240] loss: 0.002695\n",
            "[5,   260] loss: 0.002829\n",
            "[5,   280] loss: 0.002658\n",
            "[5,   300] loss: 0.002509\n",
            "[5,   320] loss: 0.002674\n",
            "[5,   340] loss: 0.002854\n",
            "[5,   360] loss: 0.002865\n",
            "[5,   380] loss: 0.002401\n",
            "[5,   400] loss: 0.002795\n",
            "[5,   420] loss: 0.002766\n",
            "[5,   440] loss: 0.003125\n",
            "[5,   460] loss: 0.002840\n",
            "[6,    20] loss: 0.003492\n",
            "[6,    40] loss: 0.002853\n",
            "[6,    60] loss: 0.002370\n",
            "[6,    80] loss: 0.002559\n",
            "[6,   100] loss: 0.002880\n",
            "[6,   120] loss: 0.002845\n",
            "[6,   140] loss: 0.002629\n",
            "[6,   160] loss: 0.002430\n",
            "[6,   180] loss: 0.002669\n",
            "[6,   200] loss: 0.002551\n",
            "[6,   220] loss: 0.002581\n",
            "[6,   240] loss: 0.002383\n",
            "[6,   260] loss: 0.002517\n",
            "[6,   280] loss: 0.002398\n",
            "[6,   300] loss: 0.002743\n",
            "[6,   320] loss: 0.002668\n",
            "[6,   340] loss: 0.002663\n",
            "[6,   360] loss: 0.002431\n",
            "[6,   380] loss: 0.002451\n",
            "[6,   400] loss: 0.002502\n",
            "[6,   420] loss: 0.002586\n",
            "[6,   440] loss: 0.002770\n",
            "[6,   460] loss: 0.002516\n",
            "Loss of ensemble model 7\n",
            "[1,    20] loss: 0.022657\n",
            "[1,    40] loss: 0.020199\n",
            "[1,    60] loss: 0.016111\n",
            "[1,    80] loss: 0.013338\n",
            "[1,   100] loss: 0.012824\n",
            "[1,   120] loss: 0.010680\n",
            "[1,   140] loss: 0.009547\n",
            "[1,   160] loss: 0.008733\n",
            "[1,   180] loss: 0.008264\n",
            "[1,   200] loss: 0.007915\n",
            "[1,   220] loss: 0.007770\n",
            "[1,   240] loss: 0.007150\n",
            "[1,   260] loss: 0.006981\n",
            "[1,   280] loss: 0.006595\n",
            "[1,   300] loss: 0.006267\n",
            "[1,   320] loss: 0.006289\n",
            "[1,   340] loss: 0.006463\n",
            "[1,   360] loss: 0.005615\n",
            "[1,   380] loss: 0.005152\n",
            "[1,   400] loss: 0.005386\n",
            "[1,   420] loss: 0.005504\n",
            "[1,   440] loss: 0.005652\n",
            "[1,   460] loss: 0.005151\n",
            "[2,    20] loss: 0.006986\n",
            "[2,    40] loss: 0.005107\n",
            "[2,    60] loss: 0.004846\n",
            "[2,    80] loss: 0.004906\n",
            "[2,   100] loss: 0.004669\n",
            "[2,   120] loss: 0.004628\n",
            "[2,   140] loss: 0.004492\n",
            "[2,   160] loss: 0.004764\n",
            "[2,   180] loss: 0.003890\n",
            "[2,   200] loss: 0.004043\n",
            "[2,   220] loss: 0.004589\n",
            "[2,   240] loss: 0.004321\n",
            "[2,   260] loss: 0.004485\n",
            "[2,   280] loss: 0.004131\n",
            "[2,   300] loss: 0.004037\n",
            "[2,   320] loss: 0.003808\n",
            "[2,   340] loss: 0.003891\n",
            "[2,   360] loss: 0.003723\n",
            "[2,   380] loss: 0.003916\n",
            "[2,   400] loss: 0.003892\n",
            "[2,   420] loss: 0.003558\n",
            "[2,   440] loss: 0.003715\n",
            "[2,   460] loss: 0.003673\n",
            "[3,    20] loss: 0.005318\n",
            "[3,    40] loss: 0.003558\n",
            "[3,    60] loss: 0.003140\n",
            "[3,    80] loss: 0.003331\n",
            "[3,   100] loss: 0.003465\n",
            "[3,   120] loss: 0.003715\n",
            "[3,   140] loss: 0.003376\n",
            "[3,   160] loss: 0.003514\n",
            "[3,   180] loss: 0.003547\n",
            "[3,   200] loss: 0.003200\n",
            "[3,   220] loss: 0.003646\n",
            "[3,   240] loss: 0.003390\n",
            "[3,   260] loss: 0.003288\n",
            "[3,   280] loss: 0.003209\n",
            "[3,   300] loss: 0.003248\n",
            "[3,   320] loss: 0.003374\n",
            "[3,   340] loss: 0.003305\n",
            "[3,   360] loss: 0.003235\n",
            "[3,   380] loss: 0.003323\n",
            "[3,   400] loss: 0.003310\n",
            "[3,   420] loss: 0.003227\n",
            "[3,   440] loss: 0.003259\n",
            "[3,   460] loss: 0.003256\n",
            "[4,    20] loss: 0.004281\n",
            "[4,    40] loss: 0.003454\n",
            "[4,    60] loss: 0.003762\n",
            "[4,    80] loss: 0.002965\n",
            "[4,   100] loss: 0.003346\n",
            "[4,   120] loss: 0.003095\n",
            "[4,   140] loss: 0.003009\n",
            "[4,   160] loss: 0.002924\n",
            "[4,   180] loss: 0.003080\n",
            "[4,   200] loss: 0.003003\n",
            "[4,   220] loss: 0.003110\n",
            "[4,   240] loss: 0.003089\n",
            "[4,   260] loss: 0.002987\n",
            "[4,   280] loss: 0.002740\n",
            "[4,   300] loss: 0.003219\n",
            "[4,   320] loss: 0.002876\n",
            "[4,   340] loss: 0.002821\n",
            "[4,   360] loss: 0.002791\n",
            "[4,   380] loss: 0.002996\n",
            "[4,   400] loss: 0.002838\n",
            "[4,   420] loss: 0.002894\n",
            "[4,   440] loss: 0.002794\n",
            "[4,   460] loss: 0.003185\n",
            "[5,    20] loss: 0.004295\n",
            "[5,    40] loss: 0.002977\n",
            "[5,    60] loss: 0.002540\n",
            "[5,    80] loss: 0.002884\n",
            "[5,   100] loss: 0.002901\n",
            "[5,   120] loss: 0.002721\n",
            "[5,   140] loss: 0.002821\n",
            "[5,   160] loss: 0.003008\n",
            "[5,   180] loss: 0.002751\n",
            "[5,   200] loss: 0.002711\n",
            "[5,   220] loss: 0.002869\n",
            "[5,   240] loss: 0.002524\n",
            "[5,   260] loss: 0.002776\n",
            "[5,   280] loss: 0.002733\n",
            "[5,   300] loss: 0.002597\n",
            "[5,   320] loss: 0.002644\n",
            "[5,   340] loss: 0.002638\n",
            "[5,   360] loss: 0.002781\n",
            "[5,   380] loss: 0.002785\n",
            "[5,   400] loss: 0.002771\n",
            "[5,   420] loss: 0.002636\n",
            "[5,   440] loss: 0.002933\n",
            "[5,   460] loss: 0.002523\n",
            "[6,    20] loss: 0.004434\n",
            "[6,    40] loss: 0.002533\n",
            "[6,    60] loss: 0.002481\n",
            "[6,    80] loss: 0.002563\n",
            "[6,   100] loss: 0.002750\n",
            "[6,   120] loss: 0.002499\n",
            "[6,   140] loss: 0.002657\n",
            "[6,   160] loss: 0.002410\n",
            "[6,   180] loss: 0.002796\n",
            "[6,   200] loss: 0.002689\n",
            "[6,   220] loss: 0.002536\n",
            "[6,   240] loss: 0.002850\n",
            "[6,   260] loss: 0.002668\n",
            "[6,   280] loss: 0.002725\n",
            "[6,   300] loss: 0.002744\n",
            "[6,   320] loss: 0.002553\n",
            "[6,   340] loss: 0.002389\n",
            "[6,   360] loss: 0.003012\n",
            "[6,   380] loss: 0.002419\n",
            "[6,   400] loss: 0.002619\n",
            "[6,   420] loss: 0.002483\n",
            "[6,   440] loss: 0.002336\n",
            "[6,   460] loss: 0.002495\n",
            "Loss of ensemble model 8\n",
            "[1,    20] loss: 0.022753\n",
            "[1,    40] loss: 0.020463\n",
            "[1,    60] loss: 0.016056\n",
            "[1,    80] loss: 0.013606\n",
            "[1,   100] loss: 0.011943\n",
            "[1,   120] loss: 0.010590\n",
            "[1,   140] loss: 0.009487\n",
            "[1,   160] loss: 0.008937\n",
            "[1,   180] loss: 0.008111\n",
            "[1,   200] loss: 0.007174\n",
            "[1,   220] loss: 0.007150\n",
            "[1,   240] loss: 0.006692\n",
            "[1,   260] loss: 0.006849\n",
            "[1,   280] loss: 0.006186\n",
            "[1,   300] loss: 0.006299\n",
            "[1,   320] loss: 0.005838\n",
            "[1,   340] loss: 0.005944\n",
            "[1,   360] loss: 0.005534\n",
            "[1,   380] loss: 0.005707\n",
            "[1,   400] loss: 0.005240\n",
            "[1,   420] loss: 0.005196\n",
            "[1,   440] loss: 0.005414\n",
            "[1,   460] loss: 0.005029\n",
            "[2,    20] loss: 0.006814\n",
            "[2,    40] loss: 0.005213\n",
            "[2,    60] loss: 0.004526\n",
            "[2,    80] loss: 0.004610\n",
            "[2,   100] loss: 0.004699\n",
            "[2,   120] loss: 0.004882\n",
            "[2,   140] loss: 0.004094\n",
            "[2,   160] loss: 0.004190\n",
            "[2,   180] loss: 0.004420\n",
            "[2,   200] loss: 0.004357\n",
            "[2,   220] loss: 0.004153\n",
            "[2,   240] loss: 0.004205\n",
            "[2,   260] loss: 0.004269\n",
            "[2,   280] loss: 0.004151\n",
            "[2,   300] loss: 0.004112\n",
            "[2,   320] loss: 0.003994\n",
            "[2,   340] loss: 0.003756\n",
            "[2,   360] loss: 0.003694\n",
            "[2,   380] loss: 0.003842\n",
            "[2,   400] loss: 0.003631\n",
            "[2,   420] loss: 0.003410\n",
            "[2,   440] loss: 0.004284\n",
            "[2,   460] loss: 0.003755\n",
            "[3,    20] loss: 0.005078\n",
            "[3,    40] loss: 0.003382\n",
            "[3,    60] loss: 0.003589\n",
            "[3,    80] loss: 0.003389\n",
            "[3,   100] loss: 0.003555\n",
            "[3,   120] loss: 0.003578\n",
            "[3,   140] loss: 0.003447\n",
            "[3,   160] loss: 0.003681\n",
            "[3,   180] loss: 0.003419\n",
            "[3,   200] loss: 0.003302\n",
            "[3,   220] loss: 0.003223\n",
            "[3,   240] loss: 0.003455\n",
            "[3,   260] loss: 0.003491\n",
            "[3,   280] loss: 0.003173\n",
            "[3,   300] loss: 0.003456\n",
            "[3,   320] loss: 0.003212\n",
            "[3,   340] loss: 0.003239\n",
            "[3,   360] loss: 0.003339\n",
            "[3,   380] loss: 0.003065\n",
            "[3,   400] loss: 0.003245\n",
            "[3,   420] loss: 0.003527\n",
            "[3,   440] loss: 0.003252\n",
            "[3,   460] loss: 0.003347\n",
            "[4,    20] loss: 0.004268\n",
            "[4,    40] loss: 0.003179\n",
            "[4,    60] loss: 0.003390\n",
            "[4,    80] loss: 0.002985\n",
            "[4,   100] loss: 0.003148\n",
            "[4,   120] loss: 0.003016\n",
            "[4,   140] loss: 0.002834\n",
            "[4,   160] loss: 0.003118\n",
            "[4,   180] loss: 0.002898\n",
            "[4,   200] loss: 0.003120\n",
            "[4,   220] loss: 0.002954\n",
            "[4,   240] loss: 0.003000\n",
            "[4,   260] loss: 0.003057\n",
            "[4,   280] loss: 0.002743\n",
            "[4,   300] loss: 0.002845\n",
            "[4,   320] loss: 0.003074\n",
            "[4,   340] loss: 0.002922\n",
            "[4,   360] loss: 0.002918\n",
            "[4,   380] loss: 0.003064\n",
            "[4,   400] loss: 0.002810\n",
            "[4,   420] loss: 0.002701\n",
            "[4,   440] loss: 0.002990\n",
            "[4,   460] loss: 0.002725\n",
            "[5,    20] loss: 0.004373\n",
            "[5,    40] loss: 0.002864\n",
            "[5,    60] loss: 0.002777\n",
            "[5,    80] loss: 0.002908\n",
            "[5,   100] loss: 0.002750\n",
            "[5,   120] loss: 0.002834\n",
            "[5,   140] loss: 0.002823\n",
            "[5,   160] loss: 0.002768\n",
            "[5,   180] loss: 0.002967\n",
            "[5,   200] loss: 0.002628\n",
            "[5,   220] loss: 0.002641\n",
            "[5,   240] loss: 0.002462\n",
            "[5,   260] loss: 0.002809\n",
            "[5,   280] loss: 0.002709\n",
            "[5,   300] loss: 0.002940\n",
            "[5,   320] loss: 0.002782\n",
            "[5,   340] loss: 0.002743\n",
            "[5,   360] loss: 0.002620\n",
            "[5,   380] loss: 0.002784\n",
            "[5,   400] loss: 0.002506\n",
            "[5,   420] loss: 0.002639\n",
            "[5,   440] loss: 0.002640\n",
            "[5,   460] loss: 0.002755\n",
            "[6,    20] loss: 0.003814\n",
            "[6,    40] loss: 0.002511\n",
            "[6,    60] loss: 0.002647\n",
            "[6,    80] loss: 0.002848\n",
            "[6,   100] loss: 0.002231\n",
            "[6,   120] loss: 0.002765\n",
            "[6,   140] loss: 0.002737\n",
            "[6,   160] loss: 0.002866\n",
            "[6,   180] loss: 0.002795\n",
            "[6,   200] loss: 0.002769\n",
            "[6,   220] loss: 0.002693\n",
            "[6,   240] loss: 0.002558\n",
            "[6,   260] loss: 0.002632\n",
            "[6,   280] loss: 0.002744\n",
            "[6,   300] loss: 0.002512\n",
            "[6,   320] loss: 0.002767\n",
            "[6,   340] loss: 0.002763\n",
            "[6,   360] loss: 0.002654\n",
            "[6,   380] loss: 0.002412\n",
            "[6,   400] loss: 0.002610\n",
            "[6,   420] loss: 0.002411\n",
            "[6,   440] loss: 0.002754\n",
            "[6,   460] loss: 0.002214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sxfc4GQAhWK",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Form the encoded sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHMwONS6Ahoy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a34c5d6d-a788-423a-a084-9b1995290d0e"
      },
      "source": [
        "correct = 0\n",
        "train_data = []\n",
        "train_label = []\n",
        "for i in range(ensemble_num):\n",
        "  with torch.no_grad():\n",
        "    for data in trainloader[i]:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs=[]\n",
        "        for i in range(ensemble_num):\n",
        "          outputs.append(model[i](images))\n",
        "          outputs[i]=outputs[i].cpu().numpy()\n",
        "\n",
        "        labels = labels.cpu().numpy()\n",
        "        for i in range(len(outputs[0])):\n",
        "            d = np.concatenate((outputs[0][i], outputs[1][i]), axis=None)\n",
        "            d = np.concatenate((d, outputs[2][i]), axis=None)\n",
        "            d = np.concatenate((d, outputs[3][i]), axis=None)\n",
        "            d = np.concatenate((d, outputs[4][i]), axis=None)\n",
        "            d = np.concatenate((d, outputs[5][i]), axis=None)\n",
        "            d = np.concatenate((d, outputs[6][i]), axis=None)\n",
        "         \n",
        "            train_data.append(d)\n",
        "            train_label.append(labels[i])\n",
        "            \n",
        "print(len(train_label))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "540000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BgrQnHzH1Sp",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: Test correction rate under several Classifers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usoSmNmKH1mo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "18c59974-bca8-4c06-929a-181fe16b1a43"
      },
      "source": [
        "#Correction rate under KNeighborsClassifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.05, random_state=0)\n",
        "neigh = KNeighborsClassifier(n_neighbors=10)\n",
        "neigh.fit(X_train, y_train) \n",
        "pred = neigh.predict(X_test)\n",
        "total = 0\n",
        "correct = 1\n",
        "for i in range(len(y_test)):\n",
        "    total += 1\n",
        "    if y_test[i] == pred[i]:\n",
        "        correct += 1\n",
        "print(\"The correction rate under KNN is:\")\n",
        "print(correct * 1.0 / total * 1.0)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The correction rate under KNN is:\n",
            "0.9556296296296296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMy-cF-sKgVr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8b6cf5dd-3479-44cb-8f8a-aca9e0894006"
      },
      "source": [
        "#Correction rate under DecisionTreeClassifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.05, random_state=0)\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state=0)\n",
        "tree.fit(X_train, y_train) \n",
        "\n",
        "pred = tree.predict(X_test)\n",
        "\n",
        "total = 0\n",
        "correct = 1\n",
        "for i in range(len(y_test)):\n",
        "    total += 1\n",
        "    if y_test[i] == pred[i]:\n",
        "        correct += 1\n",
        "print(\"The correction rate under Decision Tree is:\")\n",
        "print(correct * 1.0 / total * 1.0)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The correction rate under Decision Tree is:\n",
            "0.9021111111111111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkQeQJczK3qq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "829adead-f65b-4842-f7e4-f4f86f5ab5a8"
      },
      "source": [
        "#Correction rate under RandomForestClassifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.05, random_state=0)\n",
        "\n",
        "forest = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=0)\n",
        "forest.fit(X_train, y_train) \n",
        "\n",
        "pred = forest.predict(X_test)\n",
        "total = 0\n",
        "correct = 1\n",
        "for i in range(len(y_test)):\n",
        "    total += 1\n",
        "    if y_test[i] == pred[i]:\n",
        "        correct += 1\n",
        "print(\"The correction rate under Random Forest is:\")\n",
        "print(correct * 1.0 / total * 1.0)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The correction rate under Random Forest is:\n",
            "0.8998888888888888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mm4OVFeLiUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "11916ad5-c892-4120-e116-3a62c37a357f"
      },
      "source": [
        "#Correction rate under MLPClassifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.05, random_state=0)\n",
        "\n",
        "mlp = MLPClassifier(alpha=1, max_iter=1000)\n",
        "mlp.fit(X_train, y_train) \n",
        "\n",
        "pred = mlp.predict(X_test)\n",
        "total = 0\n",
        "correct = 1\n",
        "for i in range(len(y_test)):\n",
        "    total += 1\n",
        "    if y_test[i] == pred[i]:\n",
        "        correct += 1\n",
        "print(\"The correction rate under MLPClassifier is:\")\n",
        "print(correct * 1.0 / total * 1.0)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The correction rate under MLPClassifier is:\n",
            "0.9476666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}